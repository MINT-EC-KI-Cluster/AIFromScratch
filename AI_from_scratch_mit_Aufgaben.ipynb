{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa8gSlpu8Ws8"
      },
      "source": [
        "#Module importieren\n",
        "\n",
        "**numpy:** Ermöglicht das Rechnen mit Tensoren (zb: Matrizen und Vektoren)\n",
        "\n",
        "**matplotlib:** Zum zeichnen von Graphen, Bildern, Diagrammen, ...\n",
        "\n",
        "**tensorflow:** Eigentlich zum trainieren von neuronalen Netzen, wir nutzen das Modul aber nur zum Laden des MNIST-Datensatzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O98tFj-h8QZJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "from dataclasses import dataclass #Build in Python Modul, dient lediglich der Struktur\n",
        "from typing import Tuple,List #Typangaben (Haben keine Auswirkung auf das Programm, dient nur zur Übersicht)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIteDPQA999H"
      },
      "source": [
        "#Unser eigenes Machine Learning Framework\n",
        "Im Folgendem werden wir unser eigenes Framework schreiben, mit welchem Machine Learning Modelle trainiert werden können. Unser Framework wird nach wenigen Minuten Training 80% des MNIST-Datensatzes (Handschrifterkennung) vorhersagen können"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ3YRoE2-bh0"
      },
      "source": [
        "#Aktivierungsfunktionen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Qkmrr6-2p3"
      },
      "source": [
        "Zuerst unser \"Grundgerüst\". Hier wird der \"ActivationFunction\" Datentyp definiert. Eine Activationfunktion speichert zwei Python-Funktionen, die eigentliche Funktion und die Ableitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBF80T9i-grq"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ActivationFunction:\n",
        "    function:callable\n",
        "    derivative:callable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSWB1vP_QrQ"
      },
      "source": [
        "Für unser Framework definieren wir nun Relu und Softmax als Aktivierungsfunktionen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0Hjhfzk_doi"
      },
      "outputs": [],
      "source": [
        "#Relu\n",
        "relu = ActivationFunction(lambda x: np.maximum(0, x),\n",
        "                          lambda x: x>0)\n",
        "\n",
        "_e_x = lambda x: np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "_softf = lambda x: _e_x(x) / _e_x(x).sum(axis=1, keepdims=True)\n",
        "softmax = ActivationFunction(_softf,\n",
        "                             lambda x: _softf(x) * (1 - _softf(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPWbGQZ1_t9a"
      },
      "source": [
        "#Die einzelne Schicht\n",
        "Ein Neuronales Netz besteht aus mehreren Schichten.\n",
        "In unserem Framework speichert die Klasse Layer alle Daten, die man für die mit einer Schicht verbundenen Rechnungen benötigt. (Gewichte, Biases und Aktivierungsfunktion)\n",
        "Des weiteren bietet die Klasse die Funktionen \"feedForward\" um die Neuronenwerte der nächsten Schicht zu berechnen, \"getDerivatives\" um die Ableitungen der Parameter im Bezug auf die Loss-Funktion zu berechnen und \"sgd\"(=stochastic gradient descent), um die errechneten Ableitungen zu verwenden um die Parameter an die Trainingsdaten anzupassen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj_Z_mTx_xiP"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, nActualLayer:int, nNextLayer:int, activationFunction:ActivationFunction):\n",
        "        #Gewichte und Biases mit Zufallszahlen füllen\n",
        "        self.weightsForNext = np.random.uniform(-0.5,0.5,(nActualLayer, nNextLayer))\n",
        "        self.biasesForNext = np.random.uniform(-0.5,0.5,(nNextLayer,))\n",
        "\n",
        "        #Die Aktivierungsfunktion setzen und Werteaufnahmen als leere Arrays speichern\n",
        "        self.activationFunction = activationFunction\n",
        "\n",
        "        self.last_z = np.array([])\n",
        "        self.last_values = np.array([])\n",
        "\n",
        "    def feedForward(self, neurons:np.array, recording=False):\n",
        "        \"Berechnet eine Schichtausgabe aus einer übergebenen Eingabe\"\n",
        "\n",
        "        # Wie wird z berechnet? Zugriff auf Gewichte: self.weightsForNext\n",
        "        #Zugriff aufs Bias: self.biasesForNext\n",
        "        #Zugriff auf a: neurons\n",
        "        z =\n",
        "\n",
        "        #Speichert die z-Werte und die Eingabe fürs Training\n",
        "        if recording:\n",
        "            self.last_z = z\n",
        "            self.last_values = neurons\n",
        "\n",
        "        #Zugriff Aktivierungsfunktion via self.activationFunction.function()\n",
        "        return #Was muss hier ausgegeben werden? Nutze z\n",
        "\n",
        "    def getDerivatives(self, outputDerivative:np.array) -> Tuple[np.array, np.array, np.array]:\n",
        "        #Der Code wird nur ausgeführt, wenn die nötigen Werte aufgezeichnet wurden\n",
        "        if not (len(self.last_z) and len(self.last_values)):\n",
        "            raise ValueError(\"No records\")\n",
        "\n",
        "        #Faktor f= 1/n\n",
        "        f =1/outputDerivative.shape[0]\n",
        "\n",
        "        #Zugriff auf letzte z-Werte mit self.last_z\n",
        "        #Zugriff auf letzte a-Werte mit self.last_values\n",
        "        #Zugriff auf Ableitung der Aktivierungsfunktion: self.activationFunction.derivative()\n",
        "        z =\n",
        "        b =\n",
        "        w =\n",
        "        a =\n",
        "        return b,w,a\n",
        "\n",
        "    def sgd(self, derivatives:np.array, learningRate:float) -> None:\n",
        "        \"Passt die Werte mit dem Stochastic-Descent-Optimizer an\"\n",
        "\n",
        "        #Zugriff auf Steigung des Losses in Bezug auf biases via derivatives[0]\n",
        "        #Zugriff auf Steigung des Losses in Bezug auf weights via derivates[1]\n",
        "\n",
        "        self.biasesForNext #Werte mit passendem Operator setzen (+=, -=, *=, /=)\n",
        "        self.weightsForNext #Werte mit passendem Operator setzen (+=, -=, *=, /=)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aufgaben\n",
        "1. Nutze das in der Präsentation und der numpy-Übersicht vermittelte Wissen um in der Funktion \"feedForward\" die Ausführung des Neuronalen Netzes zu programmieren\n",
        "2. Vervollständige die Funktion \"getDerivatives\", ermittel die Steigungen des Losses in Bezug auf z,b,w und a\n",
        "3. Implementiere den Operator \"Stochastic-Gradient-Descent\" in der Funktion \"sgd\"\n",
        "4. Wähle Laufzeit->Alle ausführen und folge der Ausführung des Programms. Wird dein Netz korrekt trainiert? Wenn nicht, lass dir von den Lösungen helfen"
      ],
      "metadata": {
        "id": "eUZR5szMBVuI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjDkdr6ZBpqg"
      },
      "source": [
        "#Das Netz\n",
        "Die Schichten dienen einem Neuronalem Netz. Die Klasse Network verwaltet dieses. Hierfür speichert die Klasse die einzelnen Schichten und bietet die Funktionen \"feedForward\" um Input-Daten durch das gesamte Netz zu geben und die Ausgabe zur Verfügung zu stellen, \"loss\", welche die Lossfunktion und dessen Ableitung fürs Training zur Verfügung stellt und \"training\", welche Eingabedaten und deren Labels verwendet um unser Netz zu trainieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRAQoeWiBt5x"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    def __init__(self, layers:List[Layer]):\n",
        "        self._layers = layers\n",
        "\n",
        "    def feedForward(self, inputs:np.array, recording=False):\n",
        "        #Geht alle Schichten des Netzes von vorne nach hinten durch und setzt immer die Ausgaben einer Schicht in die nächste ein\n",
        "        values = inputs\n",
        "        for layer in self._layers:\n",
        "            values = layer.feedForward(values, recording)\n",
        "\n",
        "        #Gibt die Ausgaben der letzten Schicht zurück\n",
        "        return values\n",
        "\n",
        "    def mse_loss(outputs, labels):\n",
        "        \"Berechnet den MSE-Loss\"\n",
        "\n",
        "        loss = np.sum((labels - outputs) ** 2, axis=0)\n",
        "        derivative =  (outputs-labels) * 2\n",
        "        return loss, derivative\n",
        "\n",
        "    def train(self, inputs:np.array, labels:np.array, learningRate=0.01) -> float:\n",
        "        #Das Netz durchlaufen und fürs Training nötige Werte speichern lassen\n",
        "        outputs = self.feedForward(inputs, True)\n",
        "\n",
        "        derivatives = []\n",
        "\n",
        "        #Loss errechnen\n",
        "        loss, outputDerivative = Network.mse_loss(outputs, labels)\n",
        "\n",
        "        #Netz von hinten nach vorne durchlaufen und die Steigungen in einer Liste speichern\n",
        "        for layer in self._layers[::-1]:\n",
        "            derivative = layer.getDerivatives(outputDerivative)\n",
        "            outputDerivative = derivative[2]\n",
        "            derivatives.append(derivative)\n",
        "\n",
        "        #Das Netz von hinten nach vorne durchlaufen und die Steigungen nutzen um die Parameter anzupassen\n",
        "        for layer, layerDerivatives in zip(self._layers[::-1], derivatives):\n",
        "            layer.sgd(layerDerivatives, learningRate)\n",
        "\n",
        "        #Den durchschnittlichen Loss des Batches zurückgeben\n",
        "        return sum(loss)/outputs.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_raP88C0Bj"
      },
      "source": [
        "#Das fertige Framework testen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-hfp3JPDQ4Y"
      },
      "source": [
        "##Laden der Daten\n",
        "Unser Framework ist nun fertig! Zum Testen nutzen wir das MNIST-Dataset, welches wir im Folgendem runterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftvrbpNwC4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b6fdee-7194-4545-a71c-c959c6185d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#Wertspanne liegt bei 0-255, indem wir durch 255 teilen, normalisieren wir zu 0-1\n",
        "train_images= train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZ54BZKD5Na"
      },
      "source": [
        "Roh können wir die Daten noch nicht verwenden. Wir müssen die Bilder flatten, also zweidimensionale Bilder in eindimensionale Vektoren umwandeln, damit unser Netz die Daten einlesen kann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q6m1lyRD95U"
      },
      "outputs": [],
      "source": [
        "train_images = np.array([x.flatten() for x in train_images])\n",
        "test_images = np.array([x.flatten() for x in test_images])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOmhJD6iEfyn"
      },
      "source": [
        "Schauen wir uns doch mal die Labels fürs Training an"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9LlNoJSEcIT",
        "outputId": "a0879c87-811a-4280-fe26-f175d19466a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmqVkuSSEnp6"
      },
      "source": [
        "So können wir das nicht vergleichen. Unser Neuronales Netz gibt zehn Werte aus. Jedes Output-Neuron steht für eine Zahl. Das Neuron mit dem höchsten Wert bestimmt die Zahl, die unser Netz wählt. Damit unser Netz die Labels fürs training nutzen kann, wandeln wir sie wie folgt um:\n",
        "\n",
        "0 = [1,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "1 = [0,1,0,0,0,0,0,0,0,0]\n",
        "\n",
        "2 = [0,0,1,0,0,0,0,0,0,0]\n",
        "\n",
        "usw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7u8Ta65EmyW"
      },
      "outputs": [],
      "source": [
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHwZPTQfGG8L"
      },
      "source": [
        "#Definieren eines Netzes\n",
        "Unser Netz besitzt einen Hiddenlayer mit 10 Neuronen und der Relu-Aktivierungsfunktion. Für die Output-Schicht nutzen wir die Softmax-Funktion um Wahrscheinlichkeiten für die 10 Ziffern zu definieren (Wie sicher ist sich das Modell, dass es die repräsentative Ziffer ist?)\n",
        "\n",
        "Die Bilder sind 28px*28px groß, daher gibt es 784 Inputs/Pixel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHy1Gz4PGe9X"
      },
      "outputs": [],
      "source": [
        "n = Network([\n",
        "    Layer(784, 16, relu), # 1. Schicht mit 784 Eingabepixel und 16 Ausgaben für die nächste Hidden Layer\n",
        "    Layer(16,16, relu), #2. Schicht nimmt die 16 Ausgaben der 1. Schicht an und gibt 16 für die letzte layer aus\n",
        "    Layer(16, 10, softmax) #3. Schicht nimmt die 16 Ausgaben der 2. Schicht und gibt 10 Wahrscheinlichkeiten (softmax-Funktion) aus (Für die 10 Ziffern 0-9)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vSFdNQhGyPC"
      },
      "source": [
        "##Training\n",
        "Schon können wir das Netz trainieren. Die Losses alle Epochen speichern wir in eine Liste, damit wir uns den Verlauf anschauen können.\n",
        "\n",
        "Alle 10 Epochen wird der aktuelle Loss ausgegeben"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88EKqTQhG8pN",
        "outputId": "d16313e4-e899-4feb-e269-35bcdcfd602d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:10, loss:0.8068066139481332\n",
            "epoch:20, loss:0.6950949050461174\n"
          ]
        }
      ],
      "source": [
        "epochs = 150\n",
        "\n",
        "losses = []\n",
        "for epoch in range(1,epochs+1):\n",
        "    loss =n.train(train_images, train_labels, 0.9)\n",
        "    losses.append(loss)\n",
        "    if (epoch)%10==0:\n",
        "        print(f\"epoch:{epoch}, loss:{loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Graph zeigt die Änderung des Losses während des Trainings\n",
        "Aufgrund unseres simplen SGD-Optimizers kann es zu kurzfristigen Erhöhungen des Losses kommen\n"
      ],
      "metadata": {
        "id": "zpo6nJGA4ERy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfvE-PlZHQ0M"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wie viel Prozent der nicht fürs training verwendeten Bilder erkennt unser Netz?"
      ],
      "metadata": {
        "id": "NIRLbAAT4ZS8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YECqCWDULACN"
      },
      "outputs": [],
      "source": [
        "outputs = n.feedForward(test_images)\n",
        "p = int(np.count_nonzero(np.argmax(outputs, axis=1) == test_labels)/100)\n",
        "print(f\"Percentage: {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Führe diese Zelle aus, um das Neuronale Netz ein zufälliges Bild erkennen zu lassen"
      ],
      "metadata": {
        "id": "J0hfgcPU4j5v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsX1FzWVes_b"
      },
      "outputs": [],
      "source": [
        "test_image = test_images[randint(0,len(test_images))]\n",
        "result = n.feedForward(test_image.reshape(1,784))\n",
        "print(np.argmax(result))\n",
        "\n",
        "_ = plt.imshow(test_image.reshape(28,28), cmap='gray', vmin=0, vmax=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}